{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e39063",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics and Twitter descriptions for the two artists you selected in Module 1. If the results from that pull were not to your liking, you are welcome to use the zipped data from the “Assignment Materials” section. Specifically, you are asked to do the following: \n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e65f73",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abe420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f064bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import copy\n",
    "import matplotlib as plt\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcbe6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "\n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "sw_no_punct = [word.replace(\"'\", \"\")for word in sw]\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA : \n",
    "    for em in emoji.EMOJI_DATA[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "    num_characters = sum(len(token) for token in tokens)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        fdist = FreqDist(tokens)\n",
    "        print(f\"The {num_tokens} most common tokens are:\")\n",
    "        for token, frequency in fdist.most_common(num_tokens)[:5]:\n",
    "            print(f\"{token}: {frequency}\")\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters\n",
    "            ])\n",
    "\n",
    "\n",
    "    \n",
    "def contains_emoji(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    # modify this function to remove stopwords\n",
    "    return(tokens)\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "# Splitting on whitespace rather than the book's tokenize function. That \n",
    "# function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "def tokenize(text):\n",
    "    \n",
    "    # Split the text into tokens using whitespace as the delimiter\n",
    "    #tokens = text.split()\n",
    "    \n",
    "    # Return the tokens and the modified text\n",
    "    #return tokens, ' '.join(tokens)\n",
    "    return(text.split())\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47735524",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff88201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel fre to use the below cells as an example or read in the data in a way you prefer\n",
    "\n",
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/summerpurschke/Desktop/ADS/ADS509/Mod2\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"/twitter/\"\n",
    "lyrics_folder = \"/lyrics/\"\n",
    "\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df415d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"cher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "966804cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "489ac2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                               description\n",
       "0   cher                                       NaN\n",
       "1   cher  𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data = twitter_data[['artist', 'description']]\n",
    "twitter_data.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98fa7e56",
   "metadata": {},
   "source": [
    "Read in Lyrics Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "674767d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lyrics_dictionary(folder_path):\n",
    "    lyrics_dict = {}\n",
    "\n",
    "    # Iterate over each item in the folder\n",
    "    for item_name in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item_name)\n",
    "\n",
    "        # Check if the item is a folder (artist folder)\n",
    "        if os.path.isdir(item_path):\n",
    "            artist_dict = {}\n",
    "\n",
    "            # Iterate over each file in the artist folder\n",
    "            for filename in os.listdir(item_path):\n",
    "                file_path = os.path.join(item_path, filename)\n",
    "\n",
    "                # Check if the item is a file (song file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, \"r\") as file:\n",
    "                        # Read the contents of the file\n",
    "                        lines = file.readlines()\n",
    "\n",
    "                        if lines:\n",
    "                            title = lines[0].strip()  # Assuming the first line contains the title\n",
    "                            lyrics = ''.join(lines[1:])  # Combine the remaining lines as the lyrics\n",
    "\n",
    "                            # Add the song lyrics to the artist's dictionary with the title as the inner key\n",
    "                            artist_dict[title] = lyrics\n",
    "\n",
    "            # Add the artist's dictionary to the main lyrics dictionary\n",
    "            lyrics_dict[item_name] = artist_dict\n",
    "\n",
    "    return lyrics_dict\n",
    "\n",
    "# Call the function with the lyrics folder path\n",
    "lyrics_dictionary = create_lyrics_dictionary(data_location + lyrics_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35e6b49e",
   "metadata": {},
   "source": [
    "Read in Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3073197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_files = os.listdir(data_location + twitter_folder)\n",
    "# desc_files = [f for f in twitter_files if \"followers_data\" in f]\n",
    "# twitter_data = defaultdict(list)\n",
    "\n",
    "# for f in desc_files:\n",
    "#     artist = f.split(\"_\")[0]\n",
    "\n",
    "#     with open(data_location + twitter_folder + f, 'r', encoding='utf-8', errors='ignore') as infile:\n",
    "#         next(infile)\n",
    "#         for idx, line in enumerate(infile.readlines()):\n",
    "#             line = line.strip().split(\"\\t\")\n",
    "#             if len(line) == 7:\n",
    "#                 twitter_data[artist].append(line[6])\n",
    "\n",
    "# twitter_data = dict(twitter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892d14",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords (optional)\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ca379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "# Create a df from lyrics\n",
    "#lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare,pipeline=my_pipeline)\n",
    "#lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cf534be",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec69ac9",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a5a0512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2488749</th>\n",
       "      <td>cher</td>\n",
       "      <td>•12 years old :3. •Hakuna Matata :) •iceskatin...</td>\n",
       "      <td>[•12, years, old, 3, •hakuna, matata, •iceskat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069448</th>\n",
       "      <td>cher</td>\n",
       "      <td>(☝︎ ՞ਊ ՞)☝︎</td>\n",
       "      <td>[☝︎, ՞ਊ, ՞☝︎]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370979</th>\n",
       "      <td>cher</td>\n",
       "      <td>Still I rise☀️ Pisces🌞 Libra⬆️ Virgo🌛</td>\n",
       "      <td>[still, i, rise☀️, pisces🌞, libra⬆️, virgo🌛]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231949</th>\n",
       "      <td>cher</td>\n",
       "      <td>real life angel 🤍🏹💌| ♈︎☉, ♈︎ ☾, ♌︎↑ |</td>\n",
       "      <td>[real, life, angel, 🤍🏹💌, ♈︎☉, ♈︎, ☾, ♌︎↑]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582106</th>\n",
       "      <td>cher</td>\n",
       "      <td>Careers Advisor and Secondary Teacher 😊</td>\n",
       "      <td>[careers, advisor, and, secondary, teacher, 😊]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199606</th>\n",
       "      <td>cher</td>\n",
       "      <td>Lol 🐢</td>\n",
       "      <td>[lol, 🐢]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327605</th>\n",
       "      <td>cher</td>\n",
       "      <td>Lover of older bands, especially Queen, Led Ze...</td>\n",
       "      <td>[lover, of, older, bands, especially, queen, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585740</th>\n",
       "      <td>cher</td>\n",
       "      <td>wes 🦒 i learned all my shapes from taco bell</td>\n",
       "      <td>[wes, 🦒, i, learned, all, my, shapes, from, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754272</th>\n",
       "      <td>cher</td>\n",
       "      <td>Follow your ❤️ follow your dreams and follow m...</td>\n",
       "      <td>[follow, your, ❤️, follow, your, dreams, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989996</th>\n",
       "      <td>cher</td>\n",
       "      <td>be happy 🌸</td>\n",
       "      <td>[be, happy, 🌸]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist                                        description  \\\n",
       "2488749   cher  •12 years old :3. •Hakuna Matata :) •iceskatin...   \n",
       "1069448   cher                                        (☝︎ ՞ਊ ՞)☝︎   \n",
       "370979    cher              Still I rise☀️ Pisces🌞 Libra⬆️ Virgo🌛   \n",
       "1231949   cher              real life angel 🤍🏹💌| ♈︎☉, ♈︎ ☾, ♌︎↑ |   \n",
       "582106    cher            Careers Advisor and Secondary Teacher 😊   \n",
       "1199606   cher                                              Lol 🐢   \n",
       "327605    cher  Lover of older bands, especially Queen, Led Ze...   \n",
       "2585740   cher       wes 🦒 i learned all my shapes from taco bell   \n",
       "3754272   cher  Follow your ❤️ follow your dreams and follow m...   \n",
       "989996    cher                                         be happy 🌸   \n",
       "\n",
       "                                                    tokens  \n",
       "2488749  [•12, years, old, 3, •hakuna, matata, •iceskat...  \n",
       "1069448                                      [☝︎, ՞ਊ, ՞☝︎]  \n",
       "370979        [still, i, rise☀️, pisces🌞, libra⬆️, virgo🌛]  \n",
       "1231949          [real, life, angel, 🤍🏹💌, ♈︎☉, ♈︎, ☾, ♌︎↑]  \n",
       "582106      [careers, advisor, and, secondary, teacher, 😊]  \n",
       "1199606                                           [lol, 🐢]  \n",
       "327605   [lover, of, older, bands, especially, queen, l...  \n",
       "2585740  [wes, 🦒, i, learned, all, my, shapes, from, ta...  \n",
       "3754272  [follow, your, ❤️, follow, your, dreams, and, ...  \n",
       "989996                                      [be, happy, 🌸]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c55c9",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594271",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def compare_lyrics_statistics(df1, col1, df2, col2):\n",
    "    \"\"\"\n",
    "    Compare descriptive statistics on two sets of lyrics.\n",
    "\n",
    "    Args:\n",
    "        df1 (DataFrame): The first DataFrame containing the lyrics column.\n",
    "        col1 (str): The name of the column with lyrics tokens in df1.\n",
    "        df2 (DataFrame): The second DataFrame containing the lyrics column.\n",
    "        col2 (str): The name of the column with lyrics tokens in df2.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following statistics:\n",
    "            - total_words1: Total number of words in the first set of lyrics.\n",
    "            - total_words2: Total number of words in the second set of lyrics.\n",
    "            - unique_words1: Total number of unique words in the first set of lyrics.\n",
    "            - unique_words2: Total number of unique words in the second set of lyrics.\n",
    "            - most_common_words1: List of the most common words in the first set of lyrics and their frequencies.\n",
    "            - most_common_words2: List of the most common words in the second set of lyrics and their frequencies.\n",
    "            - avg_tokens1: Average number of tokens in the first set of lyrics.\n",
    "            - avg_tokens2: Average number of tokens in the second set of lyrics.\n",
    "            - min_tokens1: Minimum number of tokens in the first set of lyrics.\n",
    "            - min_tokens2: Minimum number of tokens in the second set of lyrics.\n",
    "            - max_tokens1: Maximum number of tokens in the first set of lyrics.\n",
    "            - max_tokens2: Maximum number of tokens in the second set of lyrics.\n",
    "    \"\"\"\n",
    "    # Compile all tokens for each column into a body of text for comparison\n",
    "    lyrics1 = ' '.join(df1[col1].explode().dropna())\n",
    "    lyrics2 = ' '.join(df2[col2].explode().dropna())\n",
    "\n",
    "    # Calculate statistics for the first set of lyrics\n",
    "    tokens1 = lyrics1.split()\n",
    "    total_words1 = len(tokens1)\n",
    "    unique_words1 = len(set(tokens1))\n",
    "    word_counts1 = Counter(tokens1)\n",
    "    most_common_words1 = word_counts1.most_common()\n",
    "    avg_tokens1 = total_words1 / len(df1)\n",
    "    min_tokens1 = min(len(tokens) for tokens in df1[col1])\n",
    "    max_tokens1 = max(len(tokens) for tokens in df1[col1])\n",
    "\n",
    "    # Calculate statistics for the second set of lyrics\n",
    "    tokens2 = lyrics2.split()\n",
    "    total_words2 = len(tokens2)\n",
    "    unique_words2 = len(set(tokens2))\n",
    "    word_counts2 = Counter(tokens2)\n",
    "    most_common_words2 = word_counts2.most_common()\n",
    "    avg_tokens2 = total_words2 / len(df2)\n",
    "    min_tokens2 = min(len(tokens) for tokens in df2[col2])\n",
    "    max_tokens2 = max(len(tokens) for tokens in df2[col2])\n",
    "\n",
    "    # Return the statistics as a dictionary\n",
    "    return {\n",
    "        'total_words1': total_words1,\n",
    "        'total_words2': total_words2,\n",
    "        'unique_words1': unique_words1,\n",
    "        'unique_words2': unique_words2,\n",
    "        'most_common_words1': most_common_words1,\n",
    "        'most_common_words2': most_common_words2,\n",
    "        'avg_tokens1': avg_tokens1,\n",
    "        'avg_tokens2': avg_tokens2,\n",
    "        'min_tokens1': min_tokens1,\n",
    "        'min_tokens2': min_tokens2,\n",
    "        'max_tokens1': max_tokens1,\n",
    "        'max_tokens2': max_tokens1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2ada9",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aa526",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f23ed487",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_cher = twitter_data.loc[twitter_data['artist'] == 'cher']\n",
    "twitter_data_robyn= twitter_data.loc[twitter_data['artist'] == 'robyn']\n",
    "\n",
    "# lyrics_data_cher = lyrics_data.loc[lyrics_data['artist'] == 'cher']\n",
    "# lyrics_data_robyn= lyrics_data.loc[lyrics_data['artist'] == 'robyn']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38477d71",
   "metadata": {},
   "source": [
    "Find the count of each unique token in each corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eec77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3bb9883",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = pd.merge(\n",
    "    count_words(twitter_data_cher).reset_index().rename(columns = {'freq':'Cher Twitter Token Count'}),\n",
    "    count_words(twitter_data_robyn).reset_index().rename(columns = {'freq':'Robyn Twitter Token Count'}),\n",
    "    #count_words(lyrics_data_cher).reset_index().rename(columns = {'freq':'Cher Lyrics Token Count'}), \n",
    "    #count_words(lyrics_data_robyn).reset_index().rename(columns = {'freq':'Robyn Lyrics Token Count'}), \n",
    "    how = 'outer'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3951281b",
   "metadata": {},
   "source": [
    "Calculate Concentrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5235c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts['Cher Twitter Concentration'] = token_counts.apply(\n",
    "    lambda row: row['Cher Twitter Token Count'] / token_counts['Cher Twitter Token Count'].sum(), axis=1)\n",
    "\n",
    "token_counts['Robyn Twitter Concentration'] = token_counts.apply(\n",
    "    lambda row: row['Robyn Twitter Token Count'] / token_counts['Robyn Twitter Token Count'].sum(), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04b95d7f",
   "metadata": {},
   "source": [
    "Passes Cutoff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "172507eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts['Passes Cutoff?'] = token_counts.apply(lambda row: 'Yes' if row['Cher Twitter Token Count'] > 5 and row['Robyn Twitter Token Count'] > 5 else 'No', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d50824f6",
   "metadata": {},
   "source": [
    "Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "afe23798",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts['C/R Ratio'] = token_counts.apply(lambda row: row['Cher Twitter Concentration'] / row['Robyn Twitter Concentration'], axis=1)\n",
    "token_counts['R/C Ratio'] = token_counts.apply(lambda row: row['Robyn Twitter Concentration'] / row['Cher Twitter Concentration'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af5e4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to just those that pass the cutoff\n",
    "token_counts_cutoff = token_counts.loc[token_counts['Passes Cutoff?'] == 'Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bfd6880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most unique tokens in Chers Lyrics are:\n",
      " ['grandmother', '#fbr', 'resister', 'nana', 'rbsoul', 'grandma', '#theresistance', 'hiphoprap', 'gop', 'grandchildren']\n",
      "The 10 most unique tokens in Robyns Lyrics are:\n",
      " ['sveriges', 'träning', 'brinner', 'följ', 'gärna', 'arbetar', 'varje', 'familj', 'projektledare', 'detta']\n"
     ]
    }
   ],
   "source": [
    "top_cher_tokens = token_counts_cutoff.sort_values(by='C/R Ratio', ascending=False)['token'].head(10).tolist()\n",
    "top_robyn_tokens = token_counts_cutoff.sort_values(by='R/C Ratio', ascending=False)['token'].head(10).tolist()\n",
    "\n",
    "print('The 10 most unique tokens in Chers Lyrics are:\\n', top_cher_tokens)\n",
    "print('The 10 most unique tokens in Robyns Lyrics are:\\n', top_robyn_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53526fcd",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f52b3",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "786b2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5d772886",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-aa32d1e69abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_data_cher\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_data_cher\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-52973df95955>\u001b[0m in \u001b[0;36mwordcloud\u001b[0;34m(word_freq, title, max_words, stopwords)\u001b[0m\n\u001b[1;32m     17\u001b[0m         counter = {token:freq for (token, freq) in counter.items() \n\u001b[1;32m     18\u001b[0m                               if token not in stopwords}\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# largest entry will be 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mmax_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         frequencies = [(word, freq / max_frequency)\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_df = count_words(twitter_data_cher)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)###\n",
    "wordcloud(twitter_data_cher['tokens'], max_words=100)\n",
    "plt.subplot(1,2,2)###\n",
    "wordcloud(twitter_data_cher['tokens'], max_words=100, stopwords=freq_df.head(50).index)\n",
    "#plt.tight_layout()###\n",
    "\n",
    "freq_2015_df = count_words(df[df['year']==2015])\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)###\n",
    "wordcloud(freq_2015_df['freq'], max_words=100)\n",
    "plt.subplot(1,2,2)###\n",
    "wordcloud(freq_2015_df['freq'], max_words=100, stopwords=freq_df.head(50).index)\n",
    "#plt.tight_layout()###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a2e53",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
